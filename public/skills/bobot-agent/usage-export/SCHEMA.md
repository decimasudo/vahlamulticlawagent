# Usage Export Schema

This document describes the CSV format exported by the usage-export skill.

## File Format

- **Format:** CSV (comma-separated values)
- **Encoding:** UTF-8
- **Location:** `~/.clawdbot/exports/usage/YYYY-MM-DD.csv`
- **Naming:** One file per day, named by date

## Column Definitions

| Column | Type | Description |
|--------|------|-------------|
| `timestamp_hour` | ISO 8601 datetime | The hour bucket for aggregation (UTC). All activity within this hour is summed together. Example: `2026-01-30T14:00:00+00:00` |
| `date` | Date (YYYY-MM-DD) | The calendar date, extracted from timestamp_hour. Provided for easier filtering in BI tools. |
| `hour` | Integer (0-23) | The hour of day (UTC), extracted from timestamp_hour. Useful for time-of-day analysis. |
| `session_key` | String | OpenClaw session identifier. See **Session Key Format** below. |
| `channel` | String | The messaging channel where the activity originated. Values: `signal`, `telegram`, `discord`, `webchat`, `unknown`, etc. |
| `model` | String | The AI model used. Example: `claude-opus-4-5`, `claude-sonnet-4`, `gpt-4o` |
| `provider` | String | The AI provider. Values: `anthropic`, `openai`, `google`, etc. |
| `activity_type` | String | What type of activity occurred. See **Activity Types** below. |
| `request_count` | Integer | Number of API requests in this aggregation bucket. |
| `input_tokens` | Integer | Total tokens sent to the model (prompts, context). |
| `output_tokens` | Integer | Total tokens generated by the model (responses). |
| `cache_read_tokens` | Integer | Tokens read from prompt cache (Anthropic-specific). Reduces input token costs. |
| `cache_write_tokens` | Integer | Tokens written to prompt cache (Anthropic-specific). Initial cache population. |
| `total_tokens` | Integer | Sum of all token types: input + output + cache_read + cache_write |
| `cost_usd` | Float | **Estimated** cost in USD, calculated by OpenClaw using configured pricing. See **Cost Calculation** below. |

## Session Key Format

The `session_key` identifies which conversation/context generated the activity:

| Pattern | Description |
|---------|-------------|
| `agent:main:main` | Main agent, primary session (direct human-agent chat) |
| `agent:main:signal:group:<groupId>` | Signal group chat session |
| `agent:main:telegram:group:<groupId>` | Telegram group chat session |
| `agent:main:subagent:<taskId>` | Subagent spawned for a background task |

## Activity Types

| Activity Type | Description |
|---------------|-------------|
| `chat` | Regular conversation — model generated text response without tool calls |
| `tool:<name>` | Tool usage — model invoked a tool. Examples: `tool:exec`, `tool:read`, `tool:write`, `tool:web_search`, `tool:web_fetch`, `tool:gateway`, `tool:message`, `tool:cron` |
| `other` | Activity that doesn't fit other categories (rare) |

### Common Tool Types

| Tool | Description |
|------|-------------|
| `tool:exec` | Shell command execution |
| `tool:read` | File reading |
| `tool:write` | File writing |
| `tool:edit` | File editing |
| `tool:web_search` | Web search (Brave API) |
| `tool:web_fetch` | Fetch URL content |
| `tool:gateway` | Gateway control (config, restart) |
| `tool:message` | Send message to channel |
| `tool:cron` | Cron job management |
| `tool:session_status` | Session status check |

## Token Types Explained

### No Double Counting

**Each token appears in exactly one category** — there is no overlap between input, cache_read, cache_write, and output. This means `total_tokens` is a valid sum of all four columns.

### Token Categories

| Column | What it represents | Billing rate |
|--------|-------------------|--------------|
| **input_tokens** | Prompt tokens that were NOT cached | Standard input rate |
| **output_tokens** | Response tokens generated by the model | Standard output rate |
| **cache_read_tokens** | Prompt tokens retrieved FROM cache | ~10% of input rate |
| **cache_write_tokens** | Prompt tokens written TO cache | ~125% of input rate |

### Cache Tokens (Anthropic-specific)

Anthropic's prompt caching reduces costs by reusing previously processed context:

- **cache_read_tokens**: Tokens retrieved from cache instead of being reprocessed. Billed at a **lower rate** (~10% of input cost).
- **cache_write_tokens**: Tokens added to the cache for future reuse. Billed at a **higher rate** (~125% of input cost) but saves money on subsequent requests.

**For non-Anthropic providers:** Cache columns will be `0`.

### Why Cache Tokens Matter

- High `cache_read_tokens` = good! You're reusing context efficiently.
- High `cache_write_tokens` = normal for new conversations or after compaction.

## Cost Calculation

The `cost_usd` column is an **estimate** calculated by OpenClaw at request time, based on:

```
cost = (input_tokens × input_price) 
     + (output_tokens × output_price)
     + (cache_read_tokens × cache_read_price)
     + (cache_write_tokens × cache_write_price)
```

Where prices are from OpenClaw's configuration: `models.providers.<provider>.models[].cost`

**Caveats:**
- Prices may be outdated if config hasn't been updated
- Actual billing may differ due to rounding, promotions, or pricing changes
- This is not authoritative — check your provider dashboard for actual costs

## Example Rows

```csv
timestamp_hour,date,hour,session_key,channel,model,provider,activity_type,request_count,input_tokens,output_tokens,cache_read_tokens,cache_write_tokens,total_tokens,cost_usd
2026-01-30T14:00:00+00:00,2026-01-30,14,agent:main:main,signal,claude-opus-4-5,anthropic,chat,5,50,1200,150000,2000,153250,0.85
2026-01-30T14:00:00+00:00,2026-01-30,14,agent:main:main,signal,claude-opus-4-5,anthropic,tool:exec,12,120,800,80000,1000,81920,0.45
```

**Interpretation:** In the 14:00 UTC hour on Jan 30, there were:
- 5 chat responses (no tools) totaling ~$0.85
- 12 tool:exec calls totaling ~$0.45

## Aggregation Notes

- Data is aggregated per **hour** — all activity within the same hour is summed
- Multiple activities in one API response are counted separately (e.g., a response with text + tool call = 1 chat + 1 tool:exec)
- Token counts are split evenly if a single response has multiple activity types

## Power BI Tips

### Suggested Transformations

1. **Date/Time:** Use `timestamp_hour` as your time dimension, or split into `date` + `hour` for flexibility
2. **Costs:** Consider creating a calculated column for "cost per 1K tokens" to compare efficiency
3. **Activity Groups:** Create a custom column to group tool types (e.g., "file operations" = read + write + edit)

### Useful Measures

```dax
Total Cost = SUM('Usage'[cost_usd])
Total Tokens = SUM('Usage'[total_tokens])
Cost per 1K Tokens = DIVIDE([Total Cost], [Total Tokens] / 1000, 0)
Cache Hit Rate = DIVIDE(SUM('Usage'[cache_read_tokens]), SUM('Usage'[input_tokens]) + SUM('Usage'[cache_read_tokens]), 0)
```

---

*Schema version: 1.0.0*
*Last updated: 2026-01-30*
