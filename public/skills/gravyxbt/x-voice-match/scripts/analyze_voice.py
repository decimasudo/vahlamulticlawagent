#!/usr/bin/env python3
"""
Analyze Twitter/X account voice and extract posting patterns.
Supports both Bird CLI output and direct account analysis.
"""

import json
import re
import sys
import subprocess
from pathlib import Path
from collections import Counter
from typing import List, Dict, Any

def fetch_tweets(username: str, count: int = 50) -> List[str]:
    """Fetch tweets using Bird CLI"""
    bird_path = Path("/data/workspace/bird.sh")
    if not bird_path.exists():
        raise FileNotFoundError("Bird CLI not found at /data/workspace/bird.sh")
    
    cmd = [str(bird_path), "user-tweets", username, "-n", str(count)]
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        raise RuntimeError(f"Bird CLI failed: {result.stderr}")
    
    return parse_bird_output(result.stdout)

def parse_bird_output(output: str) -> List[Dict[str, Any]]:
    """Parse Bird CLI output into structured tweets"""
    tweets = []
    current_tweet = {}
    lines = output.split('\n')
    
    for line in lines:
        line = line.strip()
        
        # Extract username and tweet text (first line of tweet)
        if line.startswith('@') and ':' in line:
            if current_tweet and 'text' in current_tweet:
                tweets.append(current_tweet)
            username_part, text_part = line.split(':', 1)
            current_tweet = {'text': text_part.strip()}
        
        # Continue text from previous line
        elif current_tweet and 'text' in current_tweet and line and not line.startswith(('http', 'ðŸ–¼ï¸', 'ðŸ“…', 'ðŸ”—', 'â”€â”€', 'QT @', 'â”‚', 'â””â”€', 'â”Œâ”€', 'â„¹ï¸', 'ðŸŽ¬', 'ðŸ”„')):
            current_tweet['text'] += ' ' + line
        
        # Detect QT (quote tweet)
        elif 'QT @' in line or 'â”Œâ”€ QT @' in line:
            current_tweet['is_qt'] = True
        
        # Detect separator (end of tweet)
        elif line.startswith('â”€â”€â”€â”€â”€â”€'):
            if current_tweet and 'text' in current_tweet:
                tweets.append(current_tweet)
            current_tweet = {}
    
    # Add last tweet
    if current_tweet and 'text' in current_tweet:
        tweets.append(current_tweet)
    
    return tweets

def analyze_length_patterns(tweets: List[Dict]) -> Dict[str, Any]:
    """Analyze tweet length distribution"""
    lengths = [len(t['text']) for t in tweets if 'text' in t]
    
    if not lengths:
        return {"avg_length": 0, "distribution": {}}
    
    avg = sum(lengths) / len(lengths)
    short = sum(1 for l in lengths if l < 80) / len(lengths)
    medium = sum(1 for l in lengths if 80 <= l < 200) / len(lengths)
    long = sum(1 for l in lengths if l >= 200) / len(lengths)
    
    return {
        "avg_length": int(avg),
        "distribution": {
            "short": round(short, 2),
            "medium": round(medium, 2),
            "long": round(long, 2)
        }
    }

def analyze_tone(tweets: List[Dict]) -> Dict[str, Any]:
    """Detect tone markers"""
    texts = [t['text'].lower() for t in tweets if 'text' in t]
    all_text = ' '.join(texts)
    
    # Humor markers
    humor_words = ['lmao', 'lol', 'lmfao', 'fr', 'bro', 'man', 'xd']
    humor_count = sum(all_text.count(word) for word in humor_words)
    
    # Self-deprecation markers
    self_dep = ['i\'m broke', 'my bags', 'rugged', 'round trip', 'lost', 'my portfolio']
    self_dep_count = sum(all_text.count(phrase) for phrase in self_dep)
    
    # Sarcasm/irony markers
    sarcasm = ['this is fine', 'narrator:', 'totally', 'definitely', 'obviously']
    sarcasm_count = sum(all_text.count(phrase) for phrase in sarcasm)
    
    # Edge/controversial
    edge_markers = ['based', 'fr fr', 'unironically', 'hot take', 'unpopular']
    edge_count = sum(all_text.count(phrase) for phrase in edge_markers)
    
    return {
        "humor_level": "high" if humor_count > len(tweets) * 0.3 else "medium" if humor_count > 0 else "low",
        "self_deprecating": self_dep_count > 0,
        "sarcastic": sarcasm_count > 0,
        "edgy": edge_count > 0
    }

def extract_topics(tweets: List[Dict]) -> List[str]:
    """Extract main topics from tweets"""
    texts = [t['text'].lower() for t in tweets if 'text' in t]
    all_text = ' '.join(texts)
    
    topic_keywords = {
        "crypto": ["crypto", "bitcoin", "btc", "eth", "token", "coin", "defi", "pump", "rug", "bag", "liquidity"],
        "ai": ["ai", "agent", "claude", "gpt", "bot", "clawdbot", "molt", "llm"],
        "trading": ["trading", "portfolio", "buy", "sell", "hodl", "chart", "market"],
        "memes": ["meme", "fr", "lmao", "xd", "based"],
        "tech": ["tech", "code", "build", "dev", "api"],
        "current_events": ["trump", "election", "news", "breaking"],
        "nft": ["nft", "kanpai", "panda", "floor"],
        "personal": ["my", "i'm", "just bought", "need"]
    }
    
    topics = []
    for topic, keywords in topic_keywords.items():
        if sum(all_text.count(kw) for kw in keywords) > len(tweets) * 0.1:
            topics.append(topic)
    
    return topics[:5]  # Top 5 topics

def analyze_engagement_type(tweets: List[Dict]) -> str:
    """Determine primary engagement pattern"""
    qt_count = sum(1 for t in tweets if t.get('is_qt', False))
    qt_ratio = qt_count / len(tweets) if tweets else 0
    
    if qt_ratio > 0.5:
        return "reactive QT heavy"
    elif qt_ratio > 0.2:
        return "mixed original and QT"
    else:
        return "original content focused"

def extract_signature_phrases(tweets: List[Dict]) -> List[str]:
    """Find repeated phrases that define voice"""
    texts = [t['text'].lower() for t in tweets if 'text' in t]
    
    # Common signature words/phrases
    candidates = [
        'lmao', 'fr', 'bro', 'man', 'based', 'literally', 'honestly',
        'this is fine', 'lfg', 'gm', 'gn', 'ser', 'anon', 'fren',
        'ngl', 'tbh', 'imo', 'imho', 'xd'
    ]
    
    found = []
    for phrase in candidates:
        count = sum(text.count(phrase) for text in texts)
        if count >= 2:  # Used at least twice
            found.append(phrase)
    
    return found[:10]  # Top 10

def analyze_style(tweets: List[Dict]) -> Dict[str, Any]:
    """Analyze writing style markers"""
    texts = [t['text'] for t in tweets if 'text' in t]
    
    # Emoji usage
    emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]')
    emoji_count = sum(len(emoji_pattern.findall(text)) for text in texts)
    emoji_avg = emoji_count / len(texts) if texts else 0
    
    # Capitalization
    lowercase_count = sum(1 for t in texts if t and t[0].islower())
    lowercase_ratio = lowercase_count / len(texts) if texts else 0
    
    # Punctuation
    question_count = sum(text.count('?') for text in texts)
    exclaim_count = sum(text.count('!') for text in texts)
    
    return {
        "emoji_usage": "heavy" if emoji_avg > 2 else "moderate" if emoji_avg > 0.5 else "minimal",
        "capitalization": "mostly lowercase" if lowercase_ratio > 0.6 else "mixed",
        "punctuation": "casual" if (question_count + exclaim_count) < len(texts) else "expressive"
    }

def create_voice_profile(username: str, tweets: List[Dict]) -> Dict[str, Any]:
    """Create comprehensive voice profile"""
    
    length = analyze_length_patterns(tweets)
    tone = analyze_tone(tweets)
    topics = extract_topics(tweets)
    engagement = analyze_engagement_type(tweets)
    phrases = extract_signature_phrases(tweets)
    style = analyze_style(tweets)
    
    # Detect thread usage (very long tweets or numbered sequences)
    uses_threads = any(len(t.get('text', '')) > 240 for t in tweets)
    
    return {
        "account": username,
        "analyzed_tweets": len(tweets),
        "timestamp": None,  # Can add datetime.now() if needed
        "patterns": {
            **length,
            "uses_threads": uses_threads,
            "tone": tone,
            "topics": topics,
            "engagement_type": engagement,
            "signature_phrases": phrases,
            "style": style
        },
        "sample_tweets": [t['text'] for t in tweets[:5] if 'text' in t]
    }

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Analyze Twitter account voice')
    parser.add_argument('username', help='Twitter username (e.g., @gravyxbt_)')
    parser.add_argument('--tweets', type=int, default=50, help='Number of tweets to analyze')
    parser.add_argument('--input', help='Read from file instead of fetching')
    parser.add_argument('--output', default='voice-profile.json', help='Output JSON file')
    
    args = parser.parse_args()
    
    username = args.username if args.username.startswith('@') else f'@{args.username}'
    
    # Fetch or load tweets
    if args.input:
        with open(args.input, 'r') as f:
            tweets = parse_bird_output(f.read())
    else:
        print(f"Fetching {args.tweets} tweets from {username}...", file=sys.stderr)
        tweets = parse_bird_output(
            subprocess.run(
                ["/data/workspace/bird.sh", "user-tweets", username, "-n", str(args.tweets)],
                capture_output=True, text=True
            ).stdout
        )
    
    print(f"Analyzing {len(tweets)} tweets...", file=sys.stderr)
    
    # Create profile
    profile = create_voice_profile(username, tweets)
    
    # Save
    with open(args.output, 'w') as f:
        json.dump(profile, f, indent=2)
    
    print(f"\nâœ… Voice profile saved to {args.output}", file=sys.stderr)
    print(f"\nProfile Summary:", file=sys.stderr)
    print(f"  Topics: {', '.join(profile['patterns']['topics'])}", file=sys.stderr)
    print(f"  Avg Length: {profile['patterns']['avg_length']} chars", file=sys.stderr)
    print(f"  Style: {profile['patterns']['engagement_type']}", file=sys.stderr)
    print(f"  Signature: {', '.join(profile['patterns']['signature_phrases'][:5])}", file=sys.stderr)
    
    return 0

if __name__ == '__main__':
    sys.exit(main())
